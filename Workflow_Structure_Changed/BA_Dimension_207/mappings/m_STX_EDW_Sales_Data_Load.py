# Databricks notebook source
# MAGIC %run "./udf_informatica"

# COMMAND ----------


from pyspark.sql.types import *

spark.sql("use DELTA_TRAINING")
spark.sql("set spark.sql.legacy.timeParserPolicy = LEGACY")

# COMMAND ----------
%run ./MappingUtility

# COMMAND ----------
mainWorkflowId = dbutils.widgets.get("mainWorkflowId")
mainWorkflowRunId = dbutils.widgets.get("mainWorkflowRunId")
parentName = dbutils.widgets.get("parentName")
preVariableAssignment = dbutils.widgets.get("preVariableAssignment")
postVariableAssignment = dbutils.widgets.get("postVariableAssignment")
truncTargetTableOptions = dbutils.widgets.get("truncTargetTableOptions")
variablesTableName = dbutils.widgets.get("variablesTableName")

# COMMAND ----------
#Truncate Target Tables
truncateTargetTables(truncTargetTableOptions)

# COMMAND ----------
#Pre presession variable updation
updateVariable(preVariableAssignment, variablesTableName, mainWorkflowId, parentName, "m_STX_EDW_Sales_Data_Load")

# COMMAND ----------
fetchAndCreateVariables(parentName,"m_STX_EDW_Sales_Data_Load", variablesTableName, mainWorkflowId)

# COMMAND ----------
# DBTITLE 1, Shortcut_to_DW_LOAD_CONTROL_0


query_0 = f"""SELECT
  DW_LOAD_CONTROL_DT AS DW_LOAD_CONTROL_DT,
  TXN_TSTMP AS TXN_TSTMP,
  TXN_KEY_GID AS TXN_KEY_GID,
  SITE_NBR AS SITE_NBR,
  REGISTER_NBR AS REGISTER_NBR,
  TXN_NBR AS TXN_NBR,
  DS_ORDER_NBR AS DS_ORDER_NBR,
  DS_ORDER_SEQ_NBR AS DS_ORDER_SEQ_NBR,
  TXN_TYPE_ID AS TXN_TYPE_ID,
  TXN_WAS_MID_VOIDED_FLAG AS TXN_WAS_MID_VOIDED_FLAG,
  TXN_WAS_POST_VOIDED_FLAG AS TXN_WAS_POST_VOIDED_FLAG,
  COUNTRY_CD AS COUNTRY_CD,
  LOCATION_ID AS LOCATION_ID,
  LOAD_TSTMP AS LOAD_TSTMP,
  DS_CHANNEL AS DS_CHANNEL,
  DS_ASSIST_SITE_NBR AS DS_ASSIST_SITE_NBR,
  DS_ASSIST_LOCATION_ID AS DS_ASSIST_LOCATION_ID,
  DS_CURRENCY_CD AS DS_CURRENCY_CD
FROM
  DW_LOAD_CONTROL"""

df_0 = spark.sql(query_0)

df_0.createOrReplaceTempView("Shortcut_to_DW_LOAD_CONTROL_0")

# COMMAND ----------
# DBTITLE 1, SQ_Shortcut_to_DW_LOAD_CONTROL1_1


query_1 = f"""SELECT
  DW_LOAD_CONTROL_DT AS DW_LOAD_CONTROL_DT,
  NULL AS TXN_TSTMP,
  NULL AS TXN_KEY_GID,
  NULL AS SITE_NBR,
  NULL AS REGISTER_NBR,
  NULL AS TXN_NBR,
  NULL AS DS_ORDER_NBR,
  NULL AS DS_ORDER_SEQ_NBR,
  NULL AS TXN_TYPE_ID,
  NULL AS TXN_WAS_MID_VOIDED_FLAG,
  NULL AS TXN_WAS_POST_VOIDED_FLAG,
  NULL AS COUNTRY_CD,
  NULL AS LOCATION_ID,
  NULL AS LOAD_TSTMP,
  DS_CHANNEL AS DS_CHANNEL,
  NULL AS DS_ASSIST_SITE_NBR,
  NULL AS DS_ASSIST_LOCATION_ID,
  NULL AS DS_CURRENCY_CD,
  monotonically_increasing_id() AS Monotonically_Increasing_Id
FROM
  Shortcut_to_DW_LOAD_CONTROL_0
WHERE
  DW_LOAD_CONTROL_DT = TRUNC(now())"""

df_1 = spark.sql(query_1)

df_1.createOrReplaceTempView("SQ_Shortcut_to_DW_LOAD_CONTROL1_1")

# COMMAND ----------
# DBTITLE 1, EXP_STX_TRUNC_DATE_2


query_2 = f"""SELECT
  trunc(DW_LOAD_CONTROL_DT) AS O_DW_LOAD_CONTROL_DT,
  Monotonically_Increasing_Id AS Monotonically_Increasing_Id
FROM
  SQ_Shortcut_to_DW_LOAD_CONTROL1_1"""

df_2 = spark.sql(query_2)

df_2.createOrReplaceTempView("EXP_STX_TRUNC_DATE_2")

# COMMAND ----------
# DBTITLE 1, AGG_STX_COUNT_3


query_3 = f"""SELECT
  COUNT(*) AS COUNT,
  ESTD2.O_DW_LOAD_CONTROL_DT AS DW_LOAD_CONTROL_DT,
  SStDLC1.DS_CHANNEL AS DS_CHANNEL,
  last(SStDLC1.Monotonically_Increasing_Id) AS Monotonically_Increasing_Id
FROM
  SQ_Shortcut_to_DW_LOAD_CONTROL1_1 SStDLC1
  INNER JOIN EXP_STX_TRUNC_DATE_2 ESTD2 ON SStDLC1.Monotonically_Increasing_Id = ESTD2.Monotonically_Increasing_Id
GROUP BY
  O_DW_LOAD_CONTROL_DT,
  DS_CHANNEL"""

df_3 = spark.sql(query_3)

df_3.createOrReplaceTempView("AGG_STX_COUNT_3")

# COMMAND ----------
# DBTITLE 1, Shortcut_to_SALES_TRANS_TXN_4


query_4 = f"""SELECT
  DAY_DT AS DAY_DT,
  LOCATION_ID AS LOCATION_ID,
  SALES_INSTANCE_ID AS SALES_INSTANCE_ID,
  SALES_TYPE_ID AS SALES_TYPE_ID,
  VOID_TYPE_CD AS VOID_TYPE_CD,
  TXN_WAS_POST_VOIDED_FLAG AS TXN_WAS_POST_VOIDED_FLAG,
  SALES_MID_VOID_REASON_CD AS SALES_MID_VOID_REASON_CD,
  CUST_TRANS_ID AS CUST_TRANS_ID,
  TRANS_TSTMP AS TRANS_TSTMP,
  TXN_END_TSTMP AS TXN_END_TSTMP,
  REGISTER_NBR AS REGISTER_NBR,
  TRANSACTION_NBR AS TRANSACTION_NBR,
  TXN_CONTROL_ID AS TXN_CONTROL_ID,
  ORDER_NBR AS ORDER_NBR,
  ORDER_SEQ_NBR AS ORDER_SEQ_NBR,
  ORDER_CHANNEL AS ORDER_CHANNEL,
  ORDER_ASSIST_LOCATION_ID AS ORDER_ASSIST_LOCATION_ID,
  ORDER_FULFILLMENT_CHANNEL AS ORDER_FULFILLMENT_CHANNEL,
  ORDER_CREATION_CHANNEL AS ORDER_CREATION_CHANNEL,
  ORDER_CREATION_DEVICE_TYPE AS ORDER_CREATION_DEVICE_TYPE,
  ORDER_CREATION_DEVICE_WIDTH AS ORDER_CREATION_DEVICE_WIDTH,
  TXN_SEGMENT AS TXN_SEGMENT,
  PAYMENT_DEVICE_TYPE AS PAYMENT_DEVICE_TYPE,
  BP_SOURCE_CD AS BP_SOURCE_CD,
  TRANS_FLAG AS TRANS_FLAG,
  SALES_CUST_CAPTURE_CD AS SALES_CUST_CAPTURE_CD,
  ZIP_CODE AS ZIP_CODE,
  EMPLOYEE_ID AS EMPLOYEE_ID,
  CASHIER_NBR AS CASHIER_NBR,
  MANAGER_NBR AS MANAGER_NBR,
  TAX_EXEMPT_ID AS TAX_EXEMPT_ID,
  COMM_TILL_FLG AS COMM_TILL_FLG,
  SPECIAL_ORD_NBR AS SPECIAL_ORD_NBR,
  PETPERK_OVERRIDE_NBR AS PETPERK_OVERRIDE_NBR,
  PETPERK_EMAIL_IND AS PETPERK_EMAIL_IND,
  PETPERK_FIRST_NAME_IND AS PETPERK_FIRST_NAME_IND,
  PETPERK_LAST_NAME_IND AS PETPERK_LAST_NAME_IND,
  PETPERK_PHONE_NBR_IND AS PETPERK_PHONE_NBR_IND,
  LOYALTY_NBR AS LOYALTY_NBR,
  LOYALTY_REDEMPTION_ID AS LOYALTY_REDEMPTION_ID,
  LUID AS LUID,
  POINTS_REDEEMED AS POINTS_REDEEMED,
  BASE_POINTS_EARNED AS BASE_POINTS_EARNED,
  BONUS_POINTS_EARNED AS BONUS_POINTS_EARNED,
  POINT_BALANCE AS POINT_BALANCE,
  POINTS_DEDUCTED AS POINTS_DEDUCTED,
  CDC_DCOL_RAW_TXT AS CDC_DCOL_RAW_TXT,
  CDC_EMAIL_ID AS CDC_EMAIL_ID,
  CDC_FIRST_NAME_ID AS CDC_FIRST_NAME_ID,
  CDC_LAST_NAME_ID AS CDC_LAST_NAME_ID,
  CDC_PHONE_NBR_ID AS CDC_PHONE_NBR_ID,
  PHONE_TYPE AS PHONE_TYPE,
  OPT_OUT_EMAIL_FLAG AS OPT_OUT_EMAIL_FLAG,
  OPT_OUT_TEXT_FLAG AS OPT_OUT_TEXT_FLAG,
  DIGITAL_RECEIPT_ANSWER_CD AS DIGITAL_RECEIPT_ANSWER_CD,
  OFFLINE_CUST_LKP_IND AS OFFLINE_CUST_LKP_IND,
  POS_OFFLINE_REASON_ID AS POS_OFFLINE_REASON_ID,
  SALES_AMT AS SALES_AMT,
  SALES_COST AS SALES_COST,
  SALES_QTY AS SALES_QTY,
  RETURN_AMT AS RETURN_AMT,
  RETURN_COST AS RETURN_COST,
  RETURN_QTY AS RETURN_QTY,
  SPECIAL_SRVC_AMT AS SPECIAL_SRVC_AMT,
  SPECIAL_SRVC_QTY AS SPECIAL_SRVC_QTY,
  NET_COUPON_AMT AS NET_COUPON_AMT,
  NET_COUPON_QTY AS NET_COUPON_QTY,
  NET_SALES_AMT AS NET_SALES_AMT,
  NET_SALES_COST AS NET_SALES_COST,
  NET_SALES_QTY AS NET_SALES_QTY,
  NET_DISC_AMT AS NET_DISC_AMT,
  NET_DISC_QTY AS NET_DISC_QTY,
  NET_MERCH_DISC_AMT AS NET_MERCH_DISC_AMT,
  NET_MERCH_DISC_QTY AS NET_MERCH_DISC_QTY,
  NET_SPECIAL_SALES_AMT AS NET_SPECIAL_SALES_AMT,
  NET_SPECIAL_SALES_QTY AS NET_SPECIAL_SALES_QTY,
  NET_SALES_TAX_AMT AS NET_SALES_TAX_AMT,
  NET_PAYMENT_AMT AS NET_PAYMENT_AMT,
  EXCH_RATE_PCT AS EXCH_RATE_PCT,
  UPDATE_TSTMP AS UPDATE_TSTMP,
  DATE_LOADED AS DATE_LOADED
FROM
  SALES_TRANS_TXN"""

df_4 = spark.sql(query_4)

df_4.createOrReplaceTempView("Shortcut_to_SALES_TRANS_TXN_4")

# COMMAND ----------
# DBTITLE 1, SQ_Shortcut_to_SALES_TRANS_TXN_5


query_5 = f"""SELECT
  NULL AS DAY_DT,
  NULL AS LOCATION_ID,
  NULL AS SALES_INSTANCE_ID,
  NULL AS SALES_TYPE_ID,
  NULL AS VOID_TYPE_CD,
  NULL AS TXN_WAS_POST_VOIDED_FLAG,
  NULL AS SALES_MID_VOID_REASON_CD,
  NULL AS CUST_TRANS_ID,
  NULL AS TRANS_TSTMP,
  NULL AS TXN_END_TSTMP,
  NULL AS REGISTER_NBR,
  NULL AS TRANSACTION_NBR,
  NULL AS TXN_CONTROL_ID,
  NULL AS ORDER_NBR,
  NULL AS ORDER_SEQ_NBR,
  ORDER_CHANNEL AS ORDER_CHANNEL,
  NULL AS ORDER_ASSIST_LOCATION_ID,
  NULL AS ORDER_FULFILLMENT_CHANNEL,
  NULL AS ORDER_CREATION_CHANNEL,
  NULL AS ORDER_CREATION_DEVICE_TYPE,
  NULL AS ORDER_CREATION_DEVICE_WIDTH,
  NULL AS TXN_SEGMENT,
  NULL AS PAYMENT_DEVICE_TYPE,
  NULL AS BP_SOURCE_CD,
  NULL AS TRANS_FLAG,
  NULL AS SALES_CUST_CAPTURE_CD,
  NULL AS ZIP_CODE,
  NULL AS EMPLOYEE_ID,
  NULL AS CASHIER_NBR,
  NULL AS MANAGER_NBR,
  NULL AS TAX_EXEMPT_ID,
  NULL AS COMM_TILL_FLG,
  NULL AS SPECIAL_ORD_NBR,
  NULL AS PETPERK_OVERRIDE_NBR,
  NULL AS PETPERK_EMAIL_IND,
  NULL AS PETPERK_FIRST_NAME_IND,
  NULL AS PETPERK_LAST_NAME_IND,
  NULL AS PETPERK_PHONE_NBR_IND,
  NULL AS LOYALTY_NBR,
  NULL AS LOYALTY_REDEMPTION_ID,
  NULL AS LUID,
  NULL AS POINTS_REDEEMED,
  NULL AS BASE_POINTS_EARNED,
  NULL AS BONUS_POINTS_EARNED,
  NULL AS POINT_BALANCE,
  NULL AS POINTS_DEDUCTED,
  NULL AS CDC_DCOL_RAW_TXT,
  NULL AS CDC_EMAIL_ID,
  NULL AS CDC_FIRST_NAME_ID,
  NULL AS CDC_LAST_NAME_ID,
  NULL AS CDC_PHONE_NBR_ID,
  NULL AS PHONE_TYPE,
  NULL AS OPT_OUT_EMAIL_FLAG,
  NULL AS OPT_OUT_TEXT_FLAG,
  NULL AS DIGITAL_RECEIPT_ANSWER_CD,
  NULL AS OFFLINE_CUST_LKP_IND,
  NULL AS POS_OFFLINE_REASON_ID,
  NULL AS SALES_AMT,
  NULL AS SALES_COST,
  NULL AS SALES_QTY,
  NULL AS RETURN_AMT,
  NULL AS RETURN_COST,
  NULL AS RETURN_QTY,
  NULL AS SPECIAL_SRVC_AMT,
  NULL AS SPECIAL_SRVC_QTY,
  NULL AS NET_COUPON_AMT,
  NULL AS NET_COUPON_QTY,
  NULL AS NET_SALES_AMT,
  NULL AS NET_SALES_COST,
  NULL AS NET_SALES_QTY,
  NULL AS NET_DISC_AMT,
  NULL AS NET_DISC_QTY,
  NULL AS NET_MERCH_DISC_AMT,
  NULL AS NET_MERCH_DISC_QTY,
  NULL AS NET_SPECIAL_SALES_AMT,
  NULL AS NET_SPECIAL_SALES_QTY,
  NULL AS NET_SALES_TAX_AMT,
  NULL AS NET_PAYMENT_AMT,
  NULL AS EXCH_RATE_PCT,
  NULL AS UPDATE_TSTMP,
  DATE_LOADED AS DATE_LOADED,
  monotonically_increasing_id() AS Monotonically_Increasing_Id
FROM
  Shortcut_to_SALES_TRANS_TXN_4
WHERE
  DATE_LOADED = CURRENT_DATE"""

df_5 = spark.sql(query_5)

df_5.createOrReplaceTempView("SQ_Shortcut_to_SALES_TRANS_TXN_5")

# COMMAND ----------
# DBTITLE 1, EXP_EDW_TRUNC_DATE_6


query_6 = f"""SELECT
  trunc(DATE_LOADED) AS O_DATE_LOADED,
  Monotonically_Increasing_Id AS Monotonically_Increasing_Id
FROM
  SQ_Shortcut_to_SALES_TRANS_TXN_5"""

df_6 = spark.sql(query_6)

df_6.createOrReplaceTempView("EXP_EDW_TRUNC_DATE_6")

# COMMAND ----------
# DBTITLE 1, AGG_EDW_COUNT_7


query_7 = f"""SELECT
  COUNT(*) AS COUNT,
  EETD6.O_DATE_LOADED AS DATE_LOADED,
  SStSTT5.ORDER_CHANNEL AS ORDER_CHANNEL,
  last(SStSTT5.Monotonically_Increasing_Id) AS Monotonically_Increasing_Id
FROM
  SQ_Shortcut_to_SALES_TRANS_TXN_5 SStSTT5
  INNER JOIN EXP_EDW_TRUNC_DATE_6 EETD6 ON SStSTT5.Monotonically_Increasing_Id = EETD6.Monotonically_Increasing_Id
GROUP BY
  O_DATE_LOADED,
  ORDER_CHANNEL"""

df_7 = spark.sql(query_7)

df_7.createOrReplaceTempView("AGG_EDW_COUNT_7")

# COMMAND ----------
# DBTITLE 1, JNR_EDW_STX_8


query_8 = f"""SELECT
  DETAIL.COUNT AS EDW_COUNT,
  DETAIL.DATE_LOADED AS EDW_DATE_LOADED,
  DETAIL.ORDER_CHANNEL AS EDW_ORDER_CHANNEL,
  MASTER.COUNT AS STX_COUNT,
  MASTER.DW_LOAD_CONTROL_DT AS STX_DW_LOAD_CONTROL_DT,
  MASTER.DS_CHANNEL AS STX_DS_CHANNEL,
  MASTER.Monotonically_Increasing_Id AS Monotonically_Increasing_Id
FROM
  AGG_STX_COUNT_3 MASTER
  INNER JOIN AGG_EDW_COUNT_7 DETAIL ON MASTER.DW_LOAD_CONTROL_DT = DETAIL.DATE_LOADED"""

df_8 = spark.sql(query_8)

df_8.createOrReplaceTempView("JNR_EDW_STX_8")

# COMMAND ----------
# DBTITLE 1, EXP_CHANNEL_CD_CALC_9


query_9 = f"""SELECT
  EDW_DATE_LOADED AS EDW_DATE_LOADED,
  now() AS BATCH_DATE,
  IFF (EDW_ORDER_CHANNEL = 'AOS', EDW_COUNT) AS EDW_AOS,
  IFF(EDW_ORDER_CHANNEL = 'ISPU', EDW_COUNT) AS EDW_ISPU,
  IFF(EDW_ORDER_CHANNEL = 'SFS', EDW_COUNT) AS EDW_SFS,
  IFF(EDW_ORDER_CHANNEL = 'STR', EDW_COUNT) AS EDW_STR,
  IFF(EDW_ORDER_CHANNEL = 'WEB', EDW_COUNT) AS EDW_WEB,
  STX_DW_LOAD_CONTROL_DT AS STX_DW_LOAD_CONTROL_DT,
  IFF(STX_DS_CHANNEL = 'AOS', STX_COUNT) AS STX_AOS,
  IFF(STX_DS_CHANNEL = 'ISPU', STX_COUNT) AS STX_ISPU,
  IFF(STX_DS_CHANNEL = 'SFS', STX_COUNT) AS STX_SFS,
  IFF(STX_DS_CHANNEL = 'STR', STX_COUNT) AS STX_STR,
  IFF(STX_DS_CHANNEL = 'WEB', STX_COUNT) AS STX_WEB,
  Monotonically_Increasing_Id AS Monotonically_Increasing_Id
FROM
  JNR_EDW_STX_8"""

df_9 = spark.sql(query_9)

df_9.createOrReplaceTempView("EXP_CHANNEL_CD_CALC_9")

# COMMAND ----------
# DBTITLE 1, AGG_EDW_STX_COUNT_10


query_10 = f"""SELECT
  BATCH_DATE AS BATCH_DATE,
  max(EDW_AOS) AS o_AOS,
  MAX(EDW_ISPU) AS o_ISPU,
  MAX(EDW_SFS) AS o_SFS,
  MAX(EDW_STR) AS o_STR,
  MAX(EDW_WEB) AS o_WEB,
  SUM(EDW_AOS + EDW_ISPU + EDW_SFS + EDW_STR + EDW_WEB) AS EDW_COUNT,
  SUM(STX_AOS + STX_ISPU + STX_SFS + STX_STR + STX_WEB) AS STX_COUNT,
  last(Monotonically_Increasing_Id) AS Monotonically_Increasing_Id
FROM
  EXP_CHANNEL_CD_CALC_9
GROUP BY
  BATCH_DATE"""

df_10 = spark.sql(query_10)

df_10.createOrReplaceTempView("AGG_EDW_STX_COUNT_10")

# COMMAND ----------
# DBTITLE 1, BATCH_LOAD_AUD_LOG


spark.sql("""INSERT INTO
  BATCH_LOAD_AUD_LOG
SELECT
  DAY_DT AS DAY_DT,
  TXN_TSTMP AS DAY_DT,
  BATCH_DATE AS BATCH_DATE,
  BATCH_DATE AS BATCH_DATE,
  BATCH_DATE AS BATCH_DATE,
  o_AOS AS AOS,
  o_ISPU AS ISPU,
  o_SFS AS SFS,
  o_STR AS STR,
  o_WEB AS WEB,
  STX_COUNT AS STX_COUNT,
  EDW_COUNT AS EDW_COUNT,
  EDW_SALES_AMT AS EDW_SALES,
  STX_SALES_AMT AS STX_SALES,
  PLAN_SALES_AMT AS PLAN_SALES,
  ACTUAL_SALES_AMT AS ACTUAL_SALES,
  SALES_VARIANCE AS SALES_VARIANCE,
  PLAN_VARIANCE AS PLAN_VARIANCE
FROM
  AGG_EDW_STX_COUNT_10""")

# COMMAND ----------
# DBTITLE 1, FLAT_DW_LOAD_CONTROL_STX


spark.sql("""INSERT INTO
  FLAT_DW_LOAD_CONTROL_STX
SELECT
  COUNT AS COUNT,
  DW_LOAD_CONTROL_DT AS DW_LOAD_CONTROL_DT,
  DS_CHANNEL AS DS_CHANNEL
FROM
  AGG_STX_COUNT_3""")

# COMMAND ----------
# DBTITLE 1, FLAT_SALES_TRANS_TXN_EDW


spark.sql("""INSERT INTO
  FLAT_SALES_TRANS_TXN_EDW
SELECT
  COUNT AS COUNT,
  DATE_LOADED AS DW_LOAD_CONTROL_DT,
  ORDER_CHANNEL AS DS_CHANNEL
FROM
  AGG_EDW_COUNT_7""")

# COMMAND ----------
#Post session variable updation
updateVariable(postVariableAssignment, variablesTableName, mainWorkflowId, parentName, "m_STX_EDW_Sales_Data_Load")

# COMMAND ----------
#Update Mapping Variables in database.
persistVariables(variablesTableName, "m_STX_EDW_Sales_Data_Load", mainWorkflowId, parentName)
