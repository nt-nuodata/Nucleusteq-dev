# Databricks notebook source
# MAGIC %run "./udf_informatica"

# COMMAND ----------


from pyspark.sql.types import *

spark.sql("use DELTA_TRAINING")
spark.sql("set spark.sql.legacy.timeParserPolicy = LEGACY")


# COMMAND ----------
# DBTITLE 1, Shortcut_to_UDH_PCH_EVENTS_0


df_0 = spark.sql("""SELECT
  PCH_EVENT_START_DT AS PCH_EVENT_START_DT,
  PCH_EVENT_END_DT AS PCH_EVENT_END_DT,
  PCH_EVENT_LEAD_AGCY_NM AS PCH_EVENT_LEAD_AGCY_NM,
  PCH_EVENT_LOC_NAME AS PCH_EVENT_LOC_NAME,
  PCH_EVENT_GROUP_NO AS PCH_EVENT_GROUP_NO,
  STORE_IND AS STORE_IND,
  STATE AS STATE,
  CITY AS CITY,
  EVENT_GOAL AS EVENT_GOAL,
  FUNDING_PROVIDED AS FUNDING_PROVIDED,
  CAT_ADOPTIONS AS CAT_ADOPTIONS,
  DOG_ADOPTIONS AS DOG_ADOPTIONS,
  OTHER_ADOPTIONS AS OTHER_ADOPTIONS,
  ACTION AS ACTION,
  USER_ID AS USER_ID,
  UPDATE_TSTMP AS UPDATE_TSTMP,
  LOAD_TSTMP AS LOAD_TSTMP
FROM
  UDH_PCH_EVENTS""")

df_0.createOrReplaceTempView("Shortcut_to_UDH_PCH_EVENTS_0")

# COMMAND ----------
# DBTITLE 1, SQ_Shortcut_to_UDH_PCH_EVENTS_1


df_1 = spark.sql("""SELECT
  PCH_EVENT_START_DT AS PCH_EVENT_START_DT,
  PCH_EVENT_END_DT AS PCH_EVENT_END_DT,
  PCH_EVENT_LEAD_AGCY_NM AS PCH_EVENT_LEAD_AGCY_NM,
  PCH_EVENT_LOC_NAME AS PCH_EVENT_LOC_NAME,
  ACTION AS ACTION,
  monotonically_increasing_id() AS Monotonically_Increasing_Id
FROM
  Shortcut_to_UDH_PCH_EVENTS_0
WHERE
  ACTION = 'Delete'""")

df_1.createOrReplaceTempView("SQ_Shortcut_to_UDH_PCH_EVENTS_1")

# COMMAND ----------
# DBTITLE 1, EXP_TRANS_2


df_2 = spark.sql("""SELECT
  PCH_EVENT_START_DT AS PCH_EVENT_START_DT,
  PCH_EVENT_END_DT AS PCH_EVENT_END_DT,
  PCH_EVENT_LEAD_AGCY_NM AS PCH_EVENT_LEAD_AGCY_NM,
  PCH_EVENT_LOC_NAME AS PCH_EVENT_LOC_NAME,
  1 AS DELETED_IND,
  SESSSTARTTIME AS LOAD_TSTMP,
  Monotonically_Increasing_Id AS Monotonically_Increasing_Id
FROM
  SQ_Shortcut_to_UDH_PCH_EVENTS_1""")

df_2.createOrReplaceTempView("EXP_TRANS_2")

# COMMAND ----------
# DBTITLE 1, UPD_TRANS_3


df_3 = spark.sql("""SELECT
  PCH_EVENT_START_DT AS PCH_EVENT_START_DT,
  PCH_EVENT_END_DT AS PCH_EVENT_END_DT,
  PCH_EVENT_LEAD_AGCY_NM AS PCH_EVENT_LEAD_AGCY_NM,
  PCH_EVENT_LOC_NAME AS PCH_EVENT_LOC_NAME,
  DELETED_IND AS DELETED_IND,
  LOAD_TSTMP AS LOAD_TSTMP,
  Monotonically_Increasing_Id AS Monotonically_Increasing_Id
FROM
  EXP_TRANS_2""")

df_3.createOrReplaceTempView("UPD_TRANS_3")

# COMMAND ----------
# DBTITLE 1, USR_PCH_EVENTS


spark.sql("""MERGE INTO USR_PCH_EVENTS AS TARGET
USING
  UPD_TRANS_3 AS SOURCE ON TARGET.PCH_EVENT_LEAD_AGCY_NM = SOURCE.PCH_EVENT_LEAD_AGCY_NM
  AND TARGET.PCH_EVENT_START_DT = SOURCE.PCH_EVENT_START_DT
  AND TARGET.PCH_EVENT_END_DT = SOURCE.PCH_EVENT_END_DT
  AND TARGET.PCH_EVENT_LOC_NM = SOURCE.PCH_EVENT_LOC_NAME
  WHEN MATCHED THEN
UPDATE
SET
  TARGET.PCH_EVENT_START_DT = SOURCE.PCH_EVENT_START_DT,
  TARGET.PCH_EVENT_END_DT = SOURCE.PCH_EVENT_END_DT,
  TARGET.PCH_EVENT_LEAD_AGCY_NM = SOURCE.PCH_EVENT_LEAD_AGCY_NM,
  TARGET.PCH_EVENT_LOC_NM = SOURCE.PCH_EVENT_LOC_NAME,
  TARGET.DELETED_IND = SOURCE.DELETED_IND,
  TARGET.UPDATE_TSTMP = SOURCE.LOAD_TSTMP""")