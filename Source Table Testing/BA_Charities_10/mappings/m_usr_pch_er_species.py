# Databricks notebook source
# MAGIC %run "./udf_informatica"

# COMMAND ----------


from pyspark.sql.types import *

spark.sql("use DELTA_TRAINING")
spark.sql("set spark.sql.legacy.timeParserPolicy = LEGACY")


# COMMAND ----------
# DBTITLE 1, Shortcut_to_UDH_PCH_ER_SPECIES_0


df_0 = spark.sql("""SELECT
  ER_SPECIES_ID AS ER_SPECIES_ID,
  ER_SPECIES_DESC AS ER_SPECIES_DESC,
  USER_ID AS USER_ID,
  UPDATE_TSTMP AS UPDATE_TSTMP,
  LOAD_TSTMP AS LOAD_TSTMP
FROM
  UDH_PCH_ER_SPECIES""")

df_0.createOrReplaceTempView("Shortcut_to_UDH_PCH_ER_SPECIES_0")

# COMMAND ----------
# DBTITLE 1, SQ_Shortcut_to_UDH_PCH_ER_SPECIES_1


df_1 = spark.sql("""SELECT
  ER_SPECIES_ID AS ER_SPECIES_ID,
  ER_SPECIES_DESC AS ER_SPECIES_DESC,
  USER_ID AS USER_ID,
  monotonically_increasing_id() AS Monotonically_Increasing_Id
FROM
  Shortcut_to_UDH_PCH_ER_SPECIES_0""")

df_1.createOrReplaceTempView("SQ_Shortcut_to_UDH_PCH_ER_SPECIES_1")

# COMMAND ----------
# DBTITLE 1, LKP_TRANS_2


df_2 = spark.sql("""SELECT
  UPES.ER_SPECIES_ID AS ER_SPECIES_ID,
  UPES.LOAD_TSTMP AS LOAD_TSTMP,
  SStUPES1.Monotonically_Increasing_Id AS Monotonically_Increasing_Id
FROM
  SQ_Shortcut_to_UDH_PCH_ER_SPECIES_1 SStUPES1
  LEFT JOIN USR_PCH_ER_SPECIES UPES ON UPES.ER_SPECIES_ID = SStUPES1.ER_SPECIES_ID""")

df_2.createOrReplaceTempView("LKP_TRANS_2")

# COMMAND ----------
# DBTITLE 1, EXP_TRANS_3


df_3 = spark.sql("""SELECT
  SStUPES1.ER_SPECIES_ID AS ER_SPECIES_ID,
  SStUPES1.ER_SPECIES_DESC AS ER_SPECIES_DESC,
  SStUPES1.USER_ID AS USER_ID,
  SESSSTARTTIME AS UPDATE_TSTMP,
  IFF(
    ISNULL(LT2.LOAD_TSTMP),
    SESSSTARTTIME,
    LT2.LOAD_TSTMP
  ) AS LOAD_TSTMP,
  IFF(ISNULL(LT2.SStUPES1.ER_SPECIES_ID), 'I', 'U') AS LOAD_FLAG,
  LT2.Monotonically_Increasing_Id AS Monotonically_Increasing_Id
FROM
  LKP_TRANS_2 LT2
  INNER JOIN SQ_Shortcut_to_UDH_PCH_ER_SPECIES_1 SStUPES1 ON LT2.Monotonically_Increasing_Id = SStUPES1.Monotonically_Increasing_Id""")

df_3.createOrReplaceTempView("EXP_TRANS_3")

# COMMAND ----------
# DBTITLE 1, UPD_ins_upd_4


df_4 = spark.sql("""SELECT
  ER_SPECIES_ID AS ER_SPECIES_ID1,
  ER_SPECIES_DESC AS ER_SPECIES_DESC1,
  USER_ID AS USER_ID1,
  UPDATE_TSTMP AS UPDATE_TSTMP,
  LOAD_TSTMP AS LOAD_TSTMP1,
  LOAD_FLAG AS LOAD_FLAG,
  Monotonically_Increasing_Id AS Monotonically_Increasing_Id,
  IFF(LOAD_FLAG = 'I', 'DD_INSERT', 'DD_UPDATE') AS UPDATE_STRATEGY_FLAG
FROM
  EXP_TRANS_3""")

df_4.createOrReplaceTempView("UPD_ins_upd_4")

# COMMAND ----------
# DBTITLE 1, USR_PCH_ER_SPECIES


spark.sql("""MERGE INTO USR_PCH_ER_SPECIES AS TARGET
USING
  UPD_ins_upd_4 AS SOURCE ON TARGET.ER_SPECIES_ID = SOURCE.ER_SPECIES_ID1
  WHEN MATCHED
  AND SOURCE.UPDATE_STRATEGY_FLAG = "DD_UPDATE" THEN
UPDATE
SET
  TARGET.ER_SPECIES_ID = SOURCE.ER_SPECIES_ID1,
  TARGET.ER_SPECIES_DESC = SOURCE.ER_SPECIES_DESC1,
  TARGET.USER_ID = SOURCE.USER_ID1,
  TARGET.UPDATE_TSTMP = SOURCE.UPDATE_TSTMP,
  TARGET.LOAD_TSTMP = SOURCE.LOAD_TSTMP1
  WHEN MATCHED
  AND SOURCE.UPDATE_STRATEGY_FLAG = "DD_DELETE"
  AND TARGET.ER_SPECIES_DESC = SOURCE.ER_SPECIES_DESC1
  AND TARGET.USER_ID = SOURCE.USER_ID1
  AND TARGET.UPDATE_TSTMP = SOURCE.UPDATE_TSTMP
  AND TARGET.LOAD_TSTMP = SOURCE.LOAD_TSTMP1 THEN DELETE
  WHEN NOT MATCHED
  AND SOURCE.UPDATE_STRATEGY_FLAG = "DD_INSERT" THEN
INSERT
  (
    TARGET.ER_SPECIES_ID,
    TARGET.ER_SPECIES_DESC,
    TARGET.USER_ID,
    TARGET.UPDATE_TSTMP,
    TARGET.LOAD_TSTMP
  )
VALUES
  (
    SOURCE.ER_SPECIES_ID1,
    SOURCE.ER_SPECIES_DESC1,
    SOURCE.USER_ID1,
    SOURCE.UPDATE_TSTMP,
    SOURCE.LOAD_TSTMP1
  )""")