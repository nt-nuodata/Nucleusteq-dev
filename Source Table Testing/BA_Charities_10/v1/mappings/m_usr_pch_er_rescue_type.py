# Databricks notebook source
# MAGIC %run "./udf_informatica"

# COMMAND ----------


from pyspark.sql.types import *

spark.sql("use DELTA_TRAINING")
spark.sql("set spark.sql.legacy.timeParserPolicy = LEGACY")


# COMMAND ----------
# DBTITLE 1, Shortcut_to_UDH_PCH_ER_RESCUE_TYPE_0


df_0 = spark.sql("""SELECT
  ER_RESCUE_TYPE_ID AS ER_RESCUE_TYPE_ID,
  ER_RESCUE_TYPE_DESC AS ER_RESCUE_TYPE_DESC,
  USER_ID AS USER_ID,
  UPDATE_TSTMP AS UPDATE_TSTMP,
  LOAD_TSTMP AS LOAD_TSTMP
FROM
  UDH_PCH_ER_RESCUE_TYPE""")

df_0.createOrReplaceTempView("Shortcut_to_UDH_PCH_ER_RESCUE_TYPE_0")

# COMMAND ----------
# DBTITLE 1, SQ_Shortcut_to_UDH_PCH_ER_RESCUE_TYPE_1


df_1 = spark.sql("""SELECT
  ER_RESCUE_TYPE_ID AS ER_RESCUE_TYPE_ID,
  ER_RESCUE_TYPE_DESC AS ER_RESCUE_TYPE_DESC,
  USER_ID AS USER_ID,
  monotonically_increasing_id() AS Monotonically_Increasing_Id
FROM
  Shortcut_to_UDH_PCH_ER_RESCUE_TYPE_0""")

df_1.createOrReplaceTempView("SQ_Shortcut_to_UDH_PCH_ER_RESCUE_TYPE_1")

# COMMAND ----------
# DBTITLE 1, LKP_TRANS_2


df_2 = spark.sql("""SELECT
  UPERT.ER_RESCUE_TYPE_ID AS ER_RESCUE_TYPE_ID,
  UPERT.LOAD_TSTMP AS LOAD_TSTMP,
  SStUPERT1.Monotonically_Increasing_Id AS Monotonically_Increasing_Id
FROM
  SQ_Shortcut_to_UDH_PCH_ER_RESCUE_TYPE_1 SStUPERT1
  LEFT JOIN USR_PCH_ER_RESCUE_TYPE UPERT ON UPERT.ER_RESCUE_TYPE_ID = SStUPERT1.ER_RESCUE_TYPE_ID""")

df_2.createOrReplaceTempView("LKP_TRANS_2")

# COMMAND ----------
# DBTITLE 1, EXP_TRANS_3


df_3 = spark.sql("""SELECT
  SStUPERT1.ER_RESCUE_TYPE_ID AS ER_RESCUE_TYPE_ID,
  SStUPERT1.ER_RESCUE_TYPE_DESC AS ER_RESCUE_TYPE_DESC,
  SStUPERT1.USER_ID AS USER_ID,
  LT2.ER_RESCUE_TYPE_ID AS lkp_ER_RESCUE_TYPE_ID,
  SESSSTARTTIME AS UPDATE_TSTMP,
  IFF(
    ISNULL(LT2.LOAD_TSTMP),
    SESSSTARTTIME,
    LT2.LOAD_TSTMP
  ) AS LOAD_TSTMP,
  IFF(ISNULL(LT2.ER_RESCUE_TYPE_ID), 'I', 'U') AS LOAD_FLAG,
  SStUPERT1.Monotonically_Increasing_Id AS Monotonically_Increasing_Id
FROM
  SQ_Shortcut_to_UDH_PCH_ER_RESCUE_TYPE_1 SStUPERT1
  INNER JOIN LKP_TRANS_2 LT2 ON SStUPERT1.Monotonically_Increasing_Id = LT2.Monotonically_Increasing_Id""")

df_3.createOrReplaceTempView("EXP_TRANS_3")

# COMMAND ----------
# DBTITLE 1, UPD_ins_upd_4


df_4 = spark.sql("""SELECT
  ER_RESCUE_TYPE_ID AS ER_RESCUE_TYPE_ID1,
  ER_RESCUE_TYPE_DESC AS ER_RESCUE_TYPE_DESC1,
  USER_ID AS USER_ID1,
  UPDATE_TSTMP AS UPDATE_TSTMP,
  LOAD_TSTMP AS LOAD_TSTMP1,
  LOAD_FLAG AS LOAD_FLAG,
  Monotonically_Increasing_Id AS Monotonically_Increasing_Id,
  IFF(LOAD_FLAG = 'I', 'DD_INSERT', 'DD_UPDATE') AS UPDATE_STRATEGY_FLAG
FROM
  EXP_TRANS_3""")

df_4.createOrReplaceTempView("UPD_ins_upd_4")

# COMMAND ----------
# DBTITLE 1, USR_PCH_ER_RESCUE_TYPE


spark.sql("""MERGE INTO USR_PCH_ER_RESCUE_TYPE AS TARGET
USING
  UPD_ins_upd_4 AS SOURCE ON TARGET.ER_RESCUE_TYPE_ID = SOURCE.ER_RESCUE_TYPE_ID1
  WHEN MATCHED
  AND SOURCE.UPDATE_STRATEGY_FLAG = "DD_UPDATE" THEN
UPDATE
SET
  TARGET.ER_RESCUE_TYPE_ID = SOURCE.ER_RESCUE_TYPE_ID1,
  TARGET.ER_RESCUE_TYPE_DESC = SOURCE.ER_RESCUE_TYPE_DESC1,
  TARGET.USER_ID = SOURCE.USER_ID1,
  TARGET.UPDATE_TSTMP = SOURCE.UPDATE_TSTMP,
  TARGET.LOAD_TSTMP = SOURCE.LOAD_TSTMP1
  WHEN MATCHED
  AND SOURCE.UPDATE_STRATEGY_FLAG = "DD_DELETE"
  AND TARGET.ER_RESCUE_TYPE_DESC = SOURCE.ER_RESCUE_TYPE_DESC1
  AND TARGET.USER_ID = SOURCE.USER_ID1
  AND TARGET.UPDATE_TSTMP = SOURCE.UPDATE_TSTMP
  AND TARGET.LOAD_TSTMP = SOURCE.LOAD_TSTMP1 THEN DELETE
  WHEN NOT MATCHED
  AND SOURCE.UPDATE_STRATEGY_FLAG = "DD_INSERT" THEN
INSERT
  (
    TARGET.ER_RESCUE_TYPE_ID,
    TARGET.ER_RESCUE_TYPE_DESC,
    TARGET.USER_ID,
    TARGET.UPDATE_TSTMP,
    TARGET.LOAD_TSTMP
  )
VALUES
  (
    SOURCE.ER_RESCUE_TYPE_ID1,
    SOURCE.ER_RESCUE_TYPE_DESC1,
    SOURCE.USER_ID1,
    SOURCE.UPDATE_TSTMP,
    SOURCE.LOAD_TSTMP1
  )""")