# Databricks notebook source
# MAGIC %run "./udf_informatica"

# COMMAND ----------


from pyspark.sql.types import *

spark.sql("use DELTA_TRAINING")
spark.sql("set spark.sql.legacy.timeParserPolicy = LEGACY")


# COMMAND ----------
# DBTITLE 1, Shortcut_to_USR_PCH_EVENTS_0


df_0 = spark.sql("""SELECT
  PCH_EVENT_START_DT AS PCH_EVENT_START_DT,
  PCH_EVENT_END_DT AS PCH_EVENT_END_DT,
  PCH_EVENT_LEAD_AGCY_NM AS PCH_EVENT_LEAD_AGCY_NM,
  PCH_EVENT_LOC_NM AS PCH_EVENT_LOC_NM,
  PCH_EVENT_GROUP_NO AS PCH_EVENT_GROUP_NO,
  STORE_IND AS STORE_IND,
  EVENT_STATE_CD AS EVENT_STATE_CD,
  EVENT_CITY AS EVENT_CITY,
  EVENT_GOAL_AMT AS EVENT_GOAL_AMT,
  FUNDING_PROVIDED_AMT AS FUNDING_PROVIDED_AMT,
  CAT_ADOPTIONS_CNT AS CAT_ADOPTIONS_CNT,
  DOG_ADOPTIONS_CNT AS DOG_ADOPTIONS_CNT,
  OTHER_ADOPTIONS_CNT AS OTHER_ADOPTIONS_CNT,
  USER_ID AS USER_ID,
  WEEK_DT AS WEEK_DT,
  FISCAL_WK AS FISCAL_WK,
  FISCAL_MO AS FISCAL_MO,
  FISCAL_QTR AS FISCAL_QTR,
  FISCAL_YR AS FISCAL_YR,
  CAL_WK AS CAL_WK,
  CAL_MO AS CAL_MO,
  CAL_QTR AS CAL_QTR,
  CAL_YR AS CAL_YR,
  DELETED_IND AS DELETED_IND,
  UPDATE_TSTMP AS UPDATE_TSTMP,
  LOAD_TSTMP AS LOAD_TSTMP
FROM
  USR_PCH_EVENTS""")

df_0.createOrReplaceTempView("Shortcut_to_USR_PCH_EVENTS_0")

# COMMAND ----------
# DBTITLE 1, SQ_Shortcut_to_USR_PCH_EVENTS_1


df_1 = spark.sql("""SELECT
  PCH_EVENT_START_DT AS PCH_EVENT_START_DT,
  PCH_EVENT_END_DT AS PCH_EVENT_END_DT,
  PCH_EVENT_LEAD_AGCY_NM AS PCH_EVENT_LEAD_AGCY_NM,
  PCH_EVENT_LOC_NM AS PCH_EVENT_LOC_NM,
  UPDATE_TSTMP AS UPDATE_TSTMP,
  monotonically_increasing_id() AS Monotonically_Increasing_Id
FROM
  Shortcut_to_USR_PCH_EVENTS_0
WHERE
  UPDATE_TSTMP > CURRENT_DATE""")

df_1.createOrReplaceTempView("SQ_Shortcut_to_USR_PCH_EVENTS_1")

# COMMAND ----------
# DBTITLE 1, UPD_DELETE_2


df_2 = spark.sql("""SELECT
  PCH_EVENT_START_DT AS PCH_EVENT_START_DT,
  PCH_EVENT_END_DT AS PCH_EVENT_END_DT,
  PCH_EVENT_LEAD_AGCY_NM AS PCH_EVENT_LEAD_AGCY_NM,
  PCH_EVENT_LOC_NM AS PCH_EVENT_LOC_NM,
  Monotonically_Increasing_Id AS Monotonically_Increasing_Id
FROM
  SQ_Shortcut_to_USR_PCH_EVENTS_1""")

df_2.createOrReplaceTempView("UPD_DELETE_2")

# COMMAND ----------
# DBTITLE 1, UDH_PCH_EVENTS


spark.sql("""MERGE INTO UDH_PCH_EVENTS AS TARGET
USING
  UPD_DELETE_2 AS SOURCE ON TARGET.PCH_EVENT_LEAD_AGCY_NM = SOURCE.PCH_EVENT_LEAD_AGCY_NM
  AND TARGET.PCH_EVENT_START_DT = SOURCE.PCH_EVENT_START_DT
  AND TARGET.PCH_EVENT_END_DT = SOURCE.PCH_EVENT_END_DT
  AND TARGET.PCH_EVENT_LOC_NAME = SOURCE.PCH_EVENT_LOC_NM
  WHEN MATCHED THEN DELETE""")