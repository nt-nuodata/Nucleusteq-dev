# Databricks notebook source
# MAGIC %run "./udf_informatica"

# COMMAND ----------


from pyspark.sql.types import *

spark.sql("use DELTA_TRAINING")
spark.sql("set spark.sql.legacy.timeParserPolicy = LEGACY")


# COMMAND ----------
# DBTITLE 1, Shortcut_to_USR_PCH_RESCUE_WAGON_0


df_0 = spark.sql("""SELECT
  RW_TRANS_DT AS RW_TRANS_DT,
  RW_ROUTE_ID AS RW_ROUTE_ID,
  RW_ROUTE_DESC AS RW_ROUTE_DESC,
  RW_SOURCE_SHELTER AS RW_SOURCE_SHELTER,
  RW_DESTINATION_SHELTER AS RW_DESTINATION_SHELTER,
  PETS_TRANSPORTED_CNT AS PETS_TRANSPORTED_CNT,
  PUPPIES_CNT AS PUPPIES_CNT,
  DOGS_CNT AS DOGS_CNT,
  USER_ID AS USER_ID,
  WEEK_DT AS WEEK_DT,
  FISCAL_WK AS FISCAL_WK,
  FISCAL_MO AS FISCAL_MO,
  FISCAL_QTR AS FISCAL_QTR,
  FISCAL_YR AS FISCAL_YR,
  CAL_WK AS CAL_WK,
  CAL_MO AS CAL_MO,
  CAL_QTR AS CAL_QTR,
  CAL_YR AS CAL_YR,
  DELETED_IND AS DELETED_IND,
  UPDATE_TSTMP AS UPDATE_TSTMP,
  LOAD_TSTMP AS LOAD_TSTMP
FROM
  USR_PCH_RESCUE_WAGON""")

df_0.createOrReplaceTempView("Shortcut_to_USR_PCH_RESCUE_WAGON_0")

# COMMAND ----------
# DBTITLE 1, SQ_Shortcut_to_USR_PCH_RESCUE_WAGON_1


df_1 = spark.sql("""SELECT
  RW_TRANS_DT AS RW_TRANS_DT,
  RW_ROUTE_ID AS RW_ROUTE_ID,
  UPDATE_TSTMP AS UPDATE_TSTMP,
  monotonically_increasing_id() AS Monotonically_Increasing_Id
FROM
  Shortcut_to_USR_PCH_RESCUE_WAGON_0
WHERE
  UPDATE_TSTMP > CURRENT_DATE""")

df_1.createOrReplaceTempView("SQ_Shortcut_to_USR_PCH_RESCUE_WAGON_1")

# COMMAND ----------
# DBTITLE 1, UPD_DELETE_2


df_2 = spark.sql("""SELECT
  RW_TRANS_DT AS RW_TRANS_DT,
  RW_ROUTE_ID AS RW_ROUTE_ID,
  Monotonically_Increasing_Id AS Monotonically_Increasing_Id
FROM
  SQ_Shortcut_to_USR_PCH_RESCUE_WAGON_1""")

df_2.createOrReplaceTempView("UPD_DELETE_2")

# COMMAND ----------
# DBTITLE 1, UDH_PCH_RESCUE_WAGON


spark.sql("""MERGE INTO UDH_PCH_RESCUE_WAGON AS TARGET
USING
  UPD_DELETE_2 AS SOURCE ON TARGET.RW_ROUTE_ID = SOURCE.RW_ROUTE_ID
  AND TARGET.RW_TRANS_DT = SOURCE.RW_TRANS_DT
  WHEN MATCHED THEN DELETE""")