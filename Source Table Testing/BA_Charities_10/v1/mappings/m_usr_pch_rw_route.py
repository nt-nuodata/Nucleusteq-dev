# Databricks notebook source
# MAGIC %run "./udf_informatica"

# COMMAND ----------


from pyspark.sql.types import *

spark.sql("use DELTA_TRAINING")
spark.sql("set spark.sql.legacy.timeParserPolicy = LEGACY")


# COMMAND ----------
# DBTITLE 1, Shortcut_to_UDH_PCH_RW_ROUTE_0


df_0 = spark.sql("""SELECT
  RW_ROUTE_ID AS RW_ROUTE_ID,
  RW_ROUTE_DESC AS RW_ROUTE_DESC,
  USER_ID AS USER_ID,
  UPDATE_TSTMP AS UPDATE_TSTMP,
  LOAD_TSTMP AS LOAD_TSTMP
FROM
  UDH_PCH_RW_ROUTE""")

df_0.createOrReplaceTempView("Shortcut_to_UDH_PCH_RW_ROUTE_0")

# COMMAND ----------
# DBTITLE 1, SQ_Shortcut_to_UDH_PCH_RW_ROUTE_1


df_1 = spark.sql("""SELECT
  RW_ROUTE_ID AS RW_ROUTE_ID,
  RW_ROUTE_DESC AS RW_ROUTE_DESC,
  USER_ID AS USER_ID,
  monotonically_increasing_id() AS Monotonically_Increasing_Id
FROM
  Shortcut_to_UDH_PCH_RW_ROUTE_0""")

df_1.createOrReplaceTempView("SQ_Shortcut_to_UDH_PCH_RW_ROUTE_1")

# COMMAND ----------
# DBTITLE 1, LKP_TRANS_2


df_2 = spark.sql("""SELECT
  UPRR.RW_ROUTE_ID AS RW_ROUTE_ID,
  UPRR.LOAD_TSTMP AS LOAD_TSTMP,
  SStUPRR1.RW_ROUTE_ID AS in_RW_ROUTE_ID,
  SStUPRR1.Monotonically_Increasing_Id AS Monotonically_Increasing_Id
FROM
  SQ_Shortcut_to_UDH_PCH_RW_ROUTE_1 SStUPRR1
  LEFT JOIN USR_PCH_RW_ROUTE UPRR ON UPRR.RW_ROUTE_ID = SStUPRR1.RW_ROUTE_ID""")

df_2.createOrReplaceTempView("LKP_TRANS_2")

# COMMAND ----------
# DBTITLE 1, EXP_TRANS_3


df_3 = spark.sql("""SELECT
  SStUPRR1.RW_ROUTE_ID AS RW_ROUTE_ID,
  SStUPRR1.RW_ROUTE_DESC AS RW_ROUTE_DESC,
  SStUPRR1.USER_ID AS USER_ID,
  SESSSTARTTIME AS UPDATE_TSTMP,
  IFF(
    ISNULL(LT2.LOAD_TSTMP),
    SESSSTARTTIME,
    LT2.LOAD_TSTMP
  ) AS LOAD_TSTMP,
  IFF(ISNULL(LT2.SStUPRR1.RW_ROUTE_ID), 'I', 'U') AS LOAD_FLAG,
  LT2.Monotonically_Increasing_Id AS Monotonically_Increasing_Id
FROM
  LKP_TRANS_2 LT2
  INNER JOIN SQ_Shortcut_to_UDH_PCH_RW_ROUTE_1 SStUPRR1 ON LT2.Monotonically_Increasing_Id = SStUPRR1.Monotonically_Increasing_Id""")

df_3.createOrReplaceTempView("EXP_TRANS_3")

# COMMAND ----------
# DBTITLE 1, UPD_ins_upd_4


df_4 = spark.sql("""SELECT
  RW_ROUTE_ID AS RW_ROUTE_ID1,
  RW_ROUTE_DESC AS RW_ROUTE_DESC1,
  USER_ID AS USER_ID1,
  UPDATE_TSTMP AS UPDATE_TSTMP,
  LOAD_TSTMP AS LOAD_TSTMP1,
  LOAD_FLAG AS LOAD_FLAG,
  Monotonically_Increasing_Id AS Monotonically_Increasing_Id,
  IFF(LOAD_FLAG = 'I', 'DD_INSERT', 'DD_UPDATE') AS UPDATE_STRATEGY_FLAG
FROM
  EXP_TRANS_3""")

df_4.createOrReplaceTempView("UPD_ins_upd_4")

# COMMAND ----------
# DBTITLE 1, USR_PCH_RW_ROUTE


spark.sql("""MERGE INTO USR_PCH_RW_ROUTE AS TARGET
USING
  UPD_ins_upd_4 AS SOURCE ON TARGET.RW_ROUTE_ID = SOURCE.RW_ROUTE_ID1
  WHEN MATCHED
  AND SOURCE.UPDATE_STRATEGY_FLAG = "DD_UPDATE" THEN
UPDATE
SET
  TARGET.RW_ROUTE_ID = SOURCE.RW_ROUTE_ID1,
  TARGET.RW_ROUTE_DESC = SOURCE.RW_ROUTE_DESC1,
  TARGET.USER_ID = SOURCE.USER_ID1,
  TARGET.UPDATE_TSTMP = SOURCE.UPDATE_TSTMP,
  TARGET.LOAD_TSTMP = SOURCE.LOAD_TSTMP1
  WHEN MATCHED
  AND SOURCE.UPDATE_STRATEGY_FLAG = "DD_DELETE"
  AND TARGET.RW_ROUTE_DESC = SOURCE.RW_ROUTE_DESC1
  AND TARGET.USER_ID = SOURCE.USER_ID1
  AND TARGET.UPDATE_TSTMP = SOURCE.UPDATE_TSTMP
  AND TARGET.LOAD_TSTMP = SOURCE.LOAD_TSTMP1 THEN DELETE
  WHEN NOT MATCHED
  AND SOURCE.UPDATE_STRATEGY_FLAG = "DD_INSERT" THEN
INSERT
  (
    TARGET.RW_ROUTE_ID,
    TARGET.RW_ROUTE_DESC,
    TARGET.USER_ID,
    TARGET.UPDATE_TSTMP,
    TARGET.LOAD_TSTMP
  )
VALUES
  (
    SOURCE.RW_ROUTE_ID1,
    SOURCE.RW_ROUTE_DESC1,
    SOURCE.USER_ID1,
    SOURCE.UPDATE_TSTMP,
    SOURCE.LOAD_TSTMP1
  )""")